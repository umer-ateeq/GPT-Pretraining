{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umer-ateeq/GPT-Pretraining/blob/main/gpt_style_transformer_134m__training_on_7b_tokens_using_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "######## Please hit \"Run all\" in Colab and it should start training in the last cell :)"
      ],
      "metadata": {
        "id": "TOUp3-T72om0"
      },
      "id": "TOUp3-T72om0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### init"
      ],
      "metadata": {
        "id": "tXfaN5JOoMSV"
      },
      "id": "tXfaN5JOoMSV"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "import math\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "RjLOSzxPPUEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f41e867-b4e1-4794-991c-3371b74156ff"
      },
      "id": "RjLOSzxPPUEa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# emb dim, n_head, layeer to increase size of model.\n",
        "GPT_CONFIG_124M = {\n",
        "    \"num_batches_per_epoch\" : 1000,\n",
        "    \"num_workers\": 2, # usually 2-4 is safer! # This controls how many subprocesses PyTorch uses to load the data in parallel.\n",
        "    \"batch_size\":64, # Llama 2 7B was trained with a batch size of 1024\n",
        "    \"context_length\":256,    #512//2, # for 50% data overlap!\n",
        "    \"Stride\":256,\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # 768/64=12 Number of attention heads  # emb_dim / batch_size =\n",
        "    \"n_layers\": 8,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}"
      ],
      "metadata": {
        "id": "EotGecy4yHB1"
      },
      "id": "EotGecy4yHB1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data prep: importing dataset, pre-tokenizing."
      ],
      "metadata": {
        "id": "6Nf_TUTPlpnt"
      },
      "id": "6Nf_TUTPlpnt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Either run the below cell to pre-tokenize your own dataset or use download these tokenized files.\n",
        "\n",
        "# for train.bin\n",
        "!gdown \"1BnKxJaCM0QsfZNIpSs45o2X4cspupHzQ\"\n",
        "\n",
        "# for validation.bin\n",
        "!gdown \"1kowxuffn3VRKGnERvSBWTTxaGULGqu9q\""
      ],
      "metadata": {
        "id": "dQUf5lBF1mAI"
      },
      "id": "dQUf5lBF1mAI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #### This code is for pre-tokenizing your own dataset, in this case it's TinyStories from HuggingFace\n",
        "# # This will take around half hour on colab's T4 GPU.\n",
        "# !pip install --upgrade datasets huggingface_hub fsspec\n",
        "# from datasets import load_dataset\n",
        "# ds = load_dataset(\"roneneldan/TinyStories\") ## this has 0.5B tokens!\n",
        "\n",
        "# import os\n",
        "# import numpy as np\n",
        "# from tqdm.auto import tqdm\n",
        "\n",
        "# enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# def process(example):\n",
        "#     ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
        "#     out = {'ids': ids, 'len': len(ids)}\n",
        "#     return out\n",
        "\n",
        "# if not os.path.exists(\"train.bin\"):\n",
        "#     tokenized = ds.map(\n",
        "#         process,\n",
        "#         remove_columns=['text'],\n",
        "#         desc=\"tokenizing the splits\",\n",
        "#         num_proc=8,\n",
        "#         )\n",
        "#     # concatenate all the ids in each dataset into one large file we can use for training\n",
        "#     for split, dset in tokenized.items():\n",
        "#         arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
        "#         filename = f'{split}.bin'\n",
        "#         dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
        "#         arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
        "#         total_batches = 1024\n",
        "\n",
        "#         idx = 0\n",
        "#         for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
        "#             # Batch together samples for faster write\n",
        "#             batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
        "#             arr_batch = np.concatenate(batch['ids'])\n",
        "#             # Write into mmap\n",
        "#             arr[idx : idx + len(arr_batch)] = arr_batch\n",
        "#             idx += len(arr_batch)\n",
        "#         arr.flush()\n"
      ],
      "metadata": {
        "id": "NvRWfKFmbLoz"
      },
      "id": "NvRWfKFmbLoz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size  = GPT_CONFIG_124M['context_length']\n",
        "batch_size = GPT_CONFIG_124M['batch_size']\n",
        "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
        "def get_batch(split):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if torch.cuda.is_available() == True:\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "-AB1fAjiqSTw"
      },
      "id": "-AB1fAjiqSTw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model(MHA,ff,ln,block,gpt, init)"
      ],
      "metadata": {
        "id": "-LioVpqkls-V"
      },
      "id": "-LioVpqkls-V"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0),  \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "F_aXnMpf9-eK"
      },
      "id": "F_aXnMpf9-eK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "-ERUahzM-Lii"
      },
      "id": "-ERUahzM-Lii",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "Tf2LT2U9-Lc6"
      },
      "id": "Tf2LT2U9-Lc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZKkAZlGD-f7M"
      },
      "id": "ZKkAZlGD-f7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "jbcp7e4c-j45"
      },
      "id": "jbcp7e4c-j45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "GXhs5fZTw2Xd"
      },
      "id": "GXhs5fZTw2Xd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(123)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "Wae1M5PvyZuJ"
      },
      "id": "Wae1M5PvyZuJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SuaB0JJJOkbF"
      },
      "id": "SuaB0JJJOkbF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate text Function"
      ],
      "metadata": {
        "id": "qK-hpMFHlzwo"
      },
      "id": "qK-hpMFHlzwo"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "# if it exceeds the supported context size then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:] # CLEVER_INDEXING == if list is [0,1,2,3] and slicing is  [-2:] then it will return [2,3]\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "    return idx\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0] # taking the size from positional embedding shape\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad(): # dont take calc backward pass\n",
        "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=50, context_size=context_size) # takes encoded text to generate new future text upto max_new_tokens\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()\n",
        "\n",
        "\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "token_ids = generate_text_simple(model=model,idx=text_to_token_ids(start_context, tokenizer).to(device),max_new_tokens=20,context_size=GPT_CONFIG_124M[\"context_length\"])\n",
        "token_ids.to(\"cuda\")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ],
      "metadata": {
        "id": "dU1OwIduyM1E"
      },
      "id": "dU1OwIduyM1E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "# torch.manual_seed(123)\n",
        "\n",
        "\n",
        "# token_ids = generate(model=model, idx=text_to_token_ids(start_context, tokenizer).to(\"cuda\"),\n",
        "#                       max_new_tokens=20, context_size=GPT_CONFIG_124M[\"context_length\"], temperature=0.5, top_k=None, eos_id=None)\n",
        "\n",
        "# print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xx4_7V3VA8ZG"
      },
      "id": "xx4_7V3VA8ZG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# context = \"hi, i am\"\n",
        "# token_ids = generate(model=model,idx=text_to_token_ids(context, tokenizer).to(\"cuda\"),\n",
        "#                       max_new_tokens=150,context_size=GPT_CONFIG_124M[\"context_length\"],top_k=250,temperature=1.4)\n",
        "# # token_ids\n",
        "# print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "vJjshVtrOqc4"
      },
      "id": "vJjshVtrOqc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation"
      ],
      "metadata": {
        "id": "iF5M7J9Sl3td"
      },
      "id": "iF5M7J9Sl3td"
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    vocab_size = GPT_CONFIG_124M['vocab_size']\n",
        "    batch_size, seq_len = input_batch.shape\n",
        "    tbatch_size, tseq_len = target_batch.shape\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.view(batch_size * seq_len, vocab_size), target_batch.view(tbatch_size * tseq_len)) #logits: shape [2, 3, 4]  logits.flatten(0, 1):shape[6, 4]\n",
        "    return loss\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches # main formula of loss loader is this: total_loss / num_batches, everything else is a error handling\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "Q879MhMQApi5"
      },
      "id": "Q879MhMQApi5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EiE91915OtW7"
      },
      "id": "EiE91915OtW7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer"
      ],
      "metadata": {
        "id": "pKnkPVKpl8tt"
      },
      "id": "pKnkPVKpl8tt"
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
        " #increase from 25000\n",
        "warmup_steps = 1000 #smoother initial train, earlier 100\n",
        "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
        "eval_iters = 500 # increased from 100\n",
        "batch_size = 32 # changed from 16, better gradient estimate\n",
        "block_size = 128 #changed from 64, capture longer range dependencies\n",
        "\n",
        "gradient_accumulation_steps = 32 # reduced from 50\n",
        "num_epochs = 5\n",
        "\n",
        "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
        "actual_steps = (num_epochs * GPT_CONFIG_124M[\"num_batches_per_epoch\"]) // gradient_accumulation_steps\n",
        "max_iters = actual_steps\n",
        "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
        "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
        "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
        "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
        "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
        "# scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ],
      "metadata": {
        "id": "2k4WJFjwWhZo"
      },
      "id": "2k4WJFjwWhZo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OD2y2vgU8xO3"
      },
      "id": "OD2y2vgU8xO3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training(train func, train, visualise)"
      ],
      "metadata": {
        "id": "cw3knXoHmDWN"
      },
      "id": "cw3knXoHmDWN"
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_loss(model, split, eval_iter, device):\n",
        "    losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(eval_iter):\n",
        "            X, y = get_batch(split)\n",
        "            loss = calc_loss_batch(X, y, model, device)\n",
        "            losses.append(loss.item())\n",
        "    model.train()\n",
        "    return np.mean(losses)"
      ],
      "metadata": {
        "id": "UZIdwOdg8tyo"
      },
      "id": "UZIdwOdg8tyo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################  lr, clipping, Mixed precision, gradient accumulation   #############################\n",
        "\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    num_epochs,\n",
        "    num_batches_per_epoch,\n",
        "    eval_freq,\n",
        "    eval_iter,\n",
        "    start_context,\n",
        "    tokenizer,\n",
        "    gradient_accumulation_steps=1,\n",
        "    precision_dtype=torch.float16\n",
        "):\n",
        "    # Track metrics\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Use always-on mixed precision (GPU-only)\n",
        "    scaler = GradScaler()\n",
        "    ctx = autocast(device_type='cuda', dtype=precision_dtype)\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx in range(num_batches_per_epoch):\n",
        "            # Get batch and move to GPU\n",
        "            input_batch, target_batch = get_batch(\"train\")\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            # Forward pass in mixed precision\n",
        "            with ctx:\n",
        "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "            # Normalize loss for accumulation\n",
        "            loss = loss / gradient_accumulation_steps # each call is 1/8.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient accumulation step\n",
        "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                scaler.step(optimizer) # unscale grads back to float32 from float16\n",
        "                scaler.update() # \tLearns whether to increase or decrease scale for next step\n",
        "                scheduler.step() # hey, update the learning rate according to the current step or epoch\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                tokens_seen += input_batch.numel() * gradient_accumulation_steps\n",
        "                global_step += 1\n",
        "\n",
        "                # Evaluation checkpoint\n",
        "                if global_step % eval_freq == 0:\n",
        "                    train_loss = estimate_loss(model, \"train\", eval_iter, device)\n",
        "                    val_loss = estimate_loss(model, \"val\", eval_iter, device)\n",
        "                    train_losses.append(train_loss)\n",
        "                    val_losses.append(val_loss)\n",
        "                    track_tokens_seen.append(tokens_seen)\n",
        "\n",
        "                    current_lr = optimizer.param_groups[0]['lr'] # for printing but can be explored!!!\n",
        "                    print(\n",
        "                        f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                        f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}, \"\n",
        "                        f\"Perplexity {math.exp(val_loss):.3f}, LR {current_lr:.6f}\"\n",
        "                    )\n",
        "\n",
        "        # Sample generation after each epoch\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n"
      ],
      "metadata": {
        "id": "VvS3Vj4A87Wc"
      },
      "id": "VvS3Vj4A87Wc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    plt.figure(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    sns.lineplot(x=epochs_seen, y=train_losses, label=\"Training loss\")\n",
        "    sns.lineplot(x=epochs_seen, y=val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    # plt.legend(loc=\"upper right\")\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    plt.twiny()\n",
        "    sns.lineplot(x=tokens_seen, y=train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    plt.xlabel(\"Tokens seen\")\n",
        "\n",
        "    # plt.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xmKJy52_LyWD"
      },
      "id": "xmKJy52_LyWD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load my weights"
      ],
      "metadata": {
        "id": "bA6greDi8zdw"
      },
      "id": "bA6greDi8zdw"
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"1huwcuzZQn-BWI_FaDEu02HnTKNw_Lxnc\"\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(torch.load(\"/content/weights.pth\", map_location=device, weights_only=True))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "5CFMV67f82sz"
      },
      "id": "5CFMV67f82sz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training!"
      ],
      "metadata": {
        "id": "YhA2aeXQLzpp"
      },
      "id": "YhA2aeXQLzpp"
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model(\n",
        "    model=model,  # Your model\n",
        "    optimizer=optimizer,  # Your optimizer\n",
        "    scheduler = scheduler,\n",
        "    device=device,\n",
        "    num_epochs=num_epochs,\n",
        "    num_batches_per_epoch=GPT_CONFIG_124M[\"num_batches_per_epoch\"],\n",
        "    eval_freq=100,\n",
        "    eval_iter=10,\n",
        "    start_context=\"I am a language model, who is \",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "### For Visualisation!!!\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n"
      ],
      "metadata": {
        "id": "Ak9ySk3NpKyd"
      },
      "id": "Ak9ySk3NpKyd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSXOgsMo90eC"
      },
      "id": "oSXOgsMo90eC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "FK7fuoyMO8TO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}