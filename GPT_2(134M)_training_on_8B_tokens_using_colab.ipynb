{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### init"
      ],
      "metadata": {
        "id": "tXfaN5JOoMSV"
      },
      "id": "tXfaN5JOoMSV"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "import math"
      ],
      "metadata": {
        "id": "RjLOSzxPPUEa"
      },
      "id": "RjLOSzxPPUEa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# emb dim, n_head, layeer to increase size of model.\n",
        "GPT_CONFIG_124M = {\n",
        "    \"num_batches_per_epoch\" : 1000,\n",
        "    \"num_workers\": 2, # usually 2-4 is safer! # This controls how many subprocesses PyTorch uses to load the data in parallel.\n",
        "    \"batch_size\":64, # Llama 2 7B was trained with a batch size of 1024\n",
        "    \"context_length\":256,    #512//2, # for 50% data overlap!\n",
        "    \"Stride\":256,\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # 768/64=12 Number of attention heads  # emb_dim / batch_size =\n",
        "    \"n_layers\": 8,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}"
      ],
      "metadata": {
        "id": "EotGecy4yHB1"
      },
      "id": "EotGecy4yHB1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading 8B tokens!"
      ],
      "metadata": {
        "id": "FK7fuoyMO8TO"
      },
      "id": "FK7fuoyMO8TO"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets huggingface_hub fsspec\n",
        "\n",
        "import datasets\n",
        "import fsspec\n",
        "import huggingface_hub\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "\n",
        "print(\"Datasets version:\", datasets.__version__)\n",
        "print(\"FSSpec version:\", fsspec.__version__)\n",
        "print(\"Huggingface_hub version:\", huggingface_hub.__version__)\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Clear the default cache\n",
        "import shutil\n",
        "cache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets\")\n",
        "if os.path.exists(cache_dir):\n",
        "    shutil.rmtree(cache_dir)\n",
        "    print(\"Default cache cleared!\")\n",
        "\n",
        "# ============ STREAMING APPROACH ============\n",
        "# Load dataset with streaming=True to avoid memory issues\n",
        "print(\"Loading dataset with streaming...\")\n",
        "ds = load_dataset(\n",
        "    \"HuggingFaceFW/fineweb-edu\",\n",
        "    split=\"train\",\n",
        "    name=\"CC-MAIN-2024-10\",\n",
        "    streaming=True  # This is the key change!\n",
        ")\n",
        "\n",
        "print(\"Dataset loaded successfully with streaming!\")\n"
      ],
      "metadata": {
        "id": "D4p51iCZO_aH"
      },
      "id": "D4p51iCZO_aH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============ PROGRESSIVE TOKENIZATION ============\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def process_streaming_dataset(dataset, max_tokens=10_000_000_000, chunk_size=10000):\n",
        "    \"\"\"\n",
        "    Process streaming dataset in chunks to avoid memory issues\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(\"train.bin\"):\n",
        "        print(f\"Creating train.bin file for up to {max_tokens:,} tokens...\")\n",
        "\n",
        "        # Pre-allocate memory-mapped file\n",
        "        dtype = np.uint16\n",
        "        arr = np.memmap(\"train.bin\", dtype=dtype, mode='w+', shape=(max_tokens,))\n",
        "\n",
        "        current_pos = 0\n",
        "        processed_examples = 0\n",
        "        chunk_buffer = []\n",
        "\n",
        "        print(\"Starting tokenization...\")\n",
        "\n",
        "        for example in tqdm(dataset, desc=\"Processing examples\"):\n",
        "            # Tokenize the text\n",
        "            ids = enc.encode_ordinary(example['text'])\n",
        "\n",
        "            # Skip very short examples (less than 10 tokens)\n",
        "            if len(ids) < 10:\n",
        "                continue\n",
        "\n",
        "            chunk_buffer.extend(ids)\n",
        "            processed_examples += 1\n",
        "\n",
        "            # Process in chunks to manage memory\n",
        "            if len(chunk_buffer) >= chunk_size:\n",
        "                # Check if we have space\n",
        "                if current_pos + len(chunk_buffer) >= max_tokens:\n",
        "                    print(f\"Reached maximum tokens limit: {max_tokens:,}\")\n",
        "                    break\n",
        "\n",
        "                # Write chunk to file\n",
        "                arr[current_pos:current_pos + len(chunk_buffer)] = chunk_buffer\n",
        "                current_pos += len(chunk_buffer)\n",
        "                chunk_buffer = []\n",
        "\n",
        "                # Memory cleanup\n",
        "                if processed_examples % 5000 == 0: #try 5000!\n",
        "                    gc.collect()\n",
        "                    print(f\"Processed {processed_examples:,} examples, {current_pos:,} tokens\")\n",
        "\n",
        "        # Write remaining buffer\n",
        "        if chunk_buffer and current_pos + len(chunk_buffer) < max_tokens:\n",
        "            arr[current_pos:current_pos + len(chunk_buffer)] = chunk_buffer\n",
        "            current_pos += len(chunk_buffer)\n",
        "\n",
        "        # Resize array to actual size\n",
        "        arr.flush()\n",
        "        del arr\n",
        "\n",
        "        # Create final properly-sized file\n",
        "        temp_arr = np.memmap(\"train.bin\", dtype=dtype, mode='r')\n",
        "        final_data = temp_arr[:current_pos]\n",
        "\n",
        "        # Save final file\n",
        "        final_arr = np.memmap(\"train_final.bin\", dtype=dtype, mode='w+', shape=(current_pos,))\n",
        "        final_arr[:] = final_data[:]\n",
        "        final_arr.flush()\n",
        "\n",
        "        # Replace original with final\n",
        "        os.rename(\"train_final.bin\", \"train.bin\")\n",
        "\n",
        "        print(f\"Tokenization complete! Final file size: {current_pos:,} tokens\")\n",
        "        return current_pos\n",
        "    else:\n",
        "        # File already exists, get its size\n",
        "        existing_data = np.memmap(\"train.bin\", dtype=np.uint16, mode='r')\n",
        "        print(f\"Using existing train.bin with {len(existing_data):,} tokens\")\n",
        "        return len(existing_data)\n",
        "\n",
        "\n",
        "# ============ USAGE ============\n",
        "# Process the dataset (this will take time but won't crash)\n",
        "total_tokens = process_streaming_dataset(\n",
        "    ds,\n",
        "    max_tokens=8_000_000_000,  # Start with 2B tokens for Colab safety\n",
        "    chunk_size= 100_000_000  #TRY 200K Process 50k tokens at a time\n",
        ")\n",
        "\n",
        "print(f\"Dataset processing complete! Total tokens: {total_tokens:,}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "grbip0-rPDpR"
      },
      "id": "grbip0-rPDpR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Prep for a smaller dataset < 1B"
      ],
      "metadata": {
        "id": "6Nf_TUTPlpnt"
      },
      "id": "6Nf_TUTPlpnt"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade datasets huggingface_hub fsspec\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Clear the default cache\n",
        "import shutil\n",
        "import os\n",
        "cache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets\")\n",
        "if os.path.exists(cache_dir):\n",
        "    shutil.rmtree(cache_dir)\n",
        "    print(\"Default cache cleared!\")\n",
        "\n",
        "# Load the dataset with a custom cache directory\n",
        "ds = load_dataset(\"roneneldan/TinyStories\", cache_dir=\"/content/hf_datasets_cache\") ## this has 0.5B tokens!\n"
      ],
      "metadata": {
        "id": "B4B_F-M832EX"
      },
      "id": "B4B_F-M832EX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def process(example):\n",
        "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
        "    out = {'ids': ids, 'len': len(ids)}\n",
        "    return out\n",
        "\n",
        "if not os.path.exists(\"train.bin\"):\n",
        "    tokenized = ds.map(\n",
        "        process,\n",
        "        remove_columns=['text'],\n",
        "        desc=\"tokenizing the splits\",\n",
        "        num_proc=8,\n",
        "        )\n",
        "    # concatenate all the ids in each dataset into one large file we can use for training\n",
        "    for split, dset in tokenized.items():\n",
        "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
        "        filename = f'{split}.bin'\n",
        "        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
        "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
        "        total_batches = 1024\n",
        "\n",
        "        idx = 0\n",
        "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
        "            # Batch together samples for faster write\n",
        "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
        "            arr_batch = np.concatenate(batch['ids'])\n",
        "            # Write into mmap\n",
        "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
        "            idx += len(arr_batch)\n",
        "        arr.flush()\n"
      ],
      "metadata": {
        "id": "NvRWfKFmbLoz"
      },
      "id": "NvRWfKFmbLoz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size  = GPT_CONFIG_124M['context_length']\n",
        "batch_size = GPT_CONFIG_124M['batch_size']\n",
        "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
        "def get_batch(split):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "    else: # '/content/drive/MyDrive/tokenized/validation.bin',\n",
        "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if torch.cuda.is_available() == True:\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "-AB1fAjiqSTw"
      },
      "id": "-AB1fAjiqSTw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model(MHA,ff,ln,block,gpt, init)"
      ],
      "metadata": {
        "id": "-LioVpqkls-V"
      },
      "id": "-LioVpqkls-V"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0),  \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "F_aXnMpf9-eK"
      },
      "id": "F_aXnMpf9-eK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "-ERUahzM-Lii"
      },
      "id": "-ERUahzM-Lii",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "Tf2LT2U9-Lc6"
      },
      "id": "Tf2LT2U9-Lc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZKkAZlGD-f7M"
      },
      "id": "ZKkAZlGD-f7M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "jbcp7e4c-j45"
      },
      "id": "jbcp7e4c-j45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "GXhs5fZTw2Xd"
      },
      "id": "GXhs5fZTw2Xd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(123)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "Wae1M5PvyZuJ"
      },
      "id": "Wae1M5PvyZuJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SuaB0JJJOkbF"
      },
      "id": "SuaB0JJJOkbF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate text Function"
      ],
      "metadata": {
        "id": "qK-hpMFHlzwo"
      },
      "id": "qK-hpMFHlzwo"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "# if it exceeds the supported context size then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:] # CLEVER_INDEXING == if list is [0,1,2,3] and slicing is  [-2:] then it will return [2,3]\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "    return idx\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0] # taking the size from positional embedding shape\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad(): # dont take calc backward pass\n",
        "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=50, context_size=context_size) # takes encoded text to generate new future text upto max_new_tokens\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()\n",
        "\n",
        "\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "token_ids = generate_text_simple(model=model,idx=text_to_token_ids(start_context, tokenizer).to(device),max_new_tokens=20,context_size=GPT_CONFIG_124M[\"context_length\"])\n",
        "token_ids.to(\"cuda\")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ],
      "metadata": {
        "id": "dU1OwIduyM1E"
      },
      "id": "dU1OwIduyM1E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "# torch.manual_seed(123)\n",
        "\n",
        "\n",
        "# token_ids = generate(model=model, idx=text_to_token_ids(start_context, tokenizer).to(\"cuda\"),\n",
        "#                       max_new_tokens=20, context_size=GPT_CONFIG_124M[\"context_length\"], temperature=0.5, top_k=None, eos_id=None)\n",
        "\n",
        "# print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xx4_7V3VA8ZG"
      },
      "id": "xx4_7V3VA8ZG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# context = \"hi, i am\"\n",
        "# token_ids = generate(model=model,idx=text_to_token_ids(context, tokenizer).to(\"cuda\"),\n",
        "#                       max_new_tokens=150,context_size=GPT_CONFIG_124M[\"context_length\"],top_k=250,temperature=1.4)\n",
        "# # token_ids\n",
        "# print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "vJjshVtrOqc4"
      },
      "id": "vJjshVtrOqc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation"
      ],
      "metadata": {
        "id": "iF5M7J9Sl3td"
      },
      "id": "iF5M7J9Sl3td"
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    vocab_size = GPT_CONFIG_124M['vocab_size']\n",
        "    batch_size, seq_len = input_batch.shape\n",
        "    tbatch_size, tseq_len = target_batch.shape\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.view(batch_size * seq_len, vocab_size), target_batch.view(tbatch_size * tseq_len)) #logits: shape [2, 3, 4]  logits.flatten(0, 1):shape[6, 4]\n",
        "    return loss\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches # main formula of loss loader is this: total_loss / num_batches, everything else is a error handling\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "Q879MhMQApi5"
      },
      "id": "Q879MhMQApi5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EiE91915OtW7"
      },
      "id": "EiE91915OtW7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer"
      ],
      "metadata": {
        "id": "pKnkPVKpl8tt"
      },
      "id": "pKnkPVKpl8tt"
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
        " #increase from 25000\n",
        "warmup_steps = 1000 #smoother initial train, earlier 100\n",
        "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
        "eval_iters = 500 # increased from 100\n",
        "batch_size = 32 # changed from 16, better gradient estimate\n",
        "block_size = 128 #changed from 64, capture longer range dependencies\n",
        "\n",
        "gradient_accumulation_steps = 32 # reduced from 50\n",
        "num_epochs = 5\n",
        "\n",
        "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
        "actual_steps = (num_epochs * GPT_CONFIG_124M[\"num_batches_per_epoch\"]) // gradient_accumulation_steps\n",
        "max_iters = actual_steps\n",
        "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
        "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
        "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
        "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
        "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
        "# scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ],
      "metadata": {
        "id": "2k4WJFjwWhZo"
      },
      "id": "2k4WJFjwWhZo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training(train func, train, visualise)"
      ],
      "metadata": {
        "id": "cw3knXoHmDWN"
      },
      "id": "cw3knXoHmDWN"
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_loss(model, split, eval_iter, device):\n",
        "    losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(eval_iter):\n",
        "            X, y = get_batch(split)\n",
        "            loss = calc_loss_batch(X, y, model, device)\n",
        "            losses.append(loss.item())\n",
        "    model.train()\n",
        "    return np.mean(losses)"
      ],
      "metadata": {
        "id": "UZIdwOdg8tyo"
      },
      "id": "UZIdwOdg8tyo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################  lr, clipping, Mixed precision, gradient accumulation   #############################\n",
        "\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    num_epochs,\n",
        "    num_batches_per_epoch,\n",
        "    eval_freq,\n",
        "    eval_iter,\n",
        "    start_context,\n",
        "    tokenizer,\n",
        "    gradient_accumulation_steps=1,\n",
        "    precision_dtype=torch.float16\n",
        "):\n",
        "    # Track metrics\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Use always-on mixed precision (GPU-only)\n",
        "    scaler = GradScaler()\n",
        "    ctx = autocast(device_type='cuda', dtype=precision_dtype)\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx in range(num_batches_per_epoch):\n",
        "            # Get batch and move to GPU\n",
        "            input_batch, target_batch = get_batch(\"train\")\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            # Forward pass in mixed precision\n",
        "            with ctx:\n",
        "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "            # Normalize loss for accumulation\n",
        "            loss = loss / gradient_accumulation_steps # each call is 1/8.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient accumulation step\n",
        "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                scaler.step(optimizer) # unscale grads back to float32 from float16\n",
        "                scaler.update() # \tLearns whether to increase or decrease scale for next step\n",
        "                scheduler.step() # hey, update the learning rate according to the current step or epoch\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                tokens_seen += input_batch.numel() * gradient_accumulation_steps\n",
        "                global_step += 1\n",
        "\n",
        "                # Evaluation checkpoint\n",
        "                if global_step % eval_freq == 0:\n",
        "                    train_loss = estimate_loss(model, \"train\", eval_iter, device)\n",
        "                    val_loss = estimate_loss(model, \"val\", eval_iter, device)\n",
        "                    train_losses.append(train_loss)\n",
        "                    val_losses.append(val_loss)\n",
        "                    track_tokens_seen.append(tokens_seen)\n",
        "\n",
        "                    current_lr = optimizer.param_groups[0]['lr'] # for printing but can be explored!!!\n",
        "                    print(\n",
        "                        f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                        f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}, \"\n",
        "                        f\"Perplexity {math.exp(val_loss):.3f}, LR {current_lr:.6f}\"\n",
        "                    )\n",
        "\n",
        "        # Sample generation after each epoch\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n"
      ],
      "metadata": {
        "id": "VvS3Vj4A87Wc"
      },
      "id": "VvS3Vj4A87Wc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    plt.figure(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    sns.lineplot(x=epochs_seen, y=train_losses, label=\"Training loss\")\n",
        "    sns.lineplot(x=epochs_seen, y=val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    # plt.legend(loc=\"upper right\")\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    plt.twiny()\n",
        "    sns.lineplot(x=tokens_seen, y=train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    plt.xlabel(\"Tokens seen\")\n",
        "\n",
        "    # plt.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xmKJy52_LyWD"
      },
      "id": "xmKJy52_LyWD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training!"
      ],
      "metadata": {
        "id": "YhA2aeXQLzpp"
      },
      "id": "YhA2aeXQLzpp"
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model(\n",
        "    model=model,  # Your model\n",
        "    optimizer=optimizer,  # Your optimizer\n",
        "    scheduler = scheduler,\n",
        "    device=device,\n",
        "    num_epochs=num_epochs,\n",
        "    num_batches_per_epoch=GPT_CONFIG_124M[\"num_batches_per_epoch\"],\n",
        "    eval_freq=100,\n",
        "    eval_iter=10,\n",
        "    start_context=\"I am a language model, who is \",\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "Ak9ySk3NpKyd"
      },
      "id": "Ak9ySk3NpKyd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### For Visualisation!!!\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n"
      ],
      "metadata": {
        "id": "-5aY7XVeQGk4"
      },
      "id": "-5aY7XVeQGk4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "FK7fuoyMO8TO"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}